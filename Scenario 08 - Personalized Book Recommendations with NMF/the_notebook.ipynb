{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1984dfd3",
   "metadata": {},
   "source": [
    "### ðŸ§  Recommendation System with NMF in Python\n",
    "Youâ€™ve been hired by BookHive, a growing online bookstore that wants to improve how users find books theyâ€™ll enjoy. The current system shows trending titles and editor picks, but it doesnâ€™t adapt to individual user preferences.\n",
    "\n",
    "Your task as a datascentist is to build a personalized book recommendation system using Non-negative Matrix Factorization (NMF). BookHive has shared a dataset of user-book ratings. These ratings are the only source of interaction data availableâ€”thereâ€™s no browsing history, no purchase logs, just how users rated different books.\n",
    "\n",
    "Using this data, you'll create a recommendation model that:\n",
    "\n",
    "- Learns hidden patterns in user preferences\n",
    "- Predicts how users might rate books they havenâ€™t seen yet\n",
    "- Suggests books each user is likely to enjoy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f66711",
   "metadata": {},
   "source": [
    "Now before we start let me tell you about the algorithm we want to use to solve the Bookhive's problem\n",
    "\n",
    "\n",
    "<img src=\"./assets/nmf.png\">\n",
    "\n",
    "**Non-negative Matrix Factorization (NMF)** is a dimensionality reduction technique commonly used in recommendation systems. It decomposes a matrix `A` into two smaller non-negative matrices, `W` and `H` whose product approximates the original matrix.\n",
    "\n",
    "Hereâ€™s how this relates to recommendation:\n",
    "\n",
    "<img src=\"./assets/matrixA.png\" height=300 width=300>\n",
    "\n",
    "Matrix \\( A \\) is the user-item rating matrix:  \n",
    "- Rows = users  \n",
    "- Columns = items  \n",
    "\n",
    "$$\n",
    "A_{ij} = \\text{rating by user } i \\text{ for item } j\n",
    "$$\n",
    "\n",
    "\n",
    "Now in this matrix most entries are missing (sparse), since users have not read all different books!\n",
    "\n",
    "\n",
    "As you know NMF approximates \\( A \\) as:\n",
    "\n",
    "$$\n",
    "A \\approx W \\cdot H\n",
    "$$\n",
    "\n",
    "- \\( W \\): user-feature matrix  \n",
    "- \\( H \\): feature-item matrix  \n",
    "\n",
    "Now if you calculate the product of the `W` and `H` the result is a **dense** matrix â€” it predicts ratings for all user-item pairs:\n",
    "\n",
    "$$\n",
    "\\hat{A} = W \\cdot H\n",
    "$$\n",
    "\n",
    "Now let's start our project by importing the needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683695c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47015b04",
   "metadata": {},
   "source": [
    "Let's import our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0782a0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2340630",
   "metadata": {},
   "source": [
    "We can see that this has three columns: `UserID`, `BookID`, and `Rating`.  \n",
    "Each row indicates how much a user has rated a specific book.\n",
    "\n",
    "Now let's find out how many unique users and unique books are in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1feab294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb4c407b",
   "metadata": {},
   "source": [
    "Now let's create our User-Item Matrix by pivoting our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6525a54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfcad31d",
   "metadata": {},
   "source": [
    "Now this is the user-item matrix. As you can see, most of the values are `NaN`, which means not all users have rated all the books.\n",
    "Let's fill this `NaN` value with `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff84e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5afcfa8c",
   "metadata": {},
   "source": [
    "This is our matrix `A`. Let's apply the NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8b9772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ed4b4d8",
   "metadata": {},
   "source": [
    "The `init` parameter in NMF plays an important role in how the algorithm starts the factorization process. by default this value is set as `random`\n",
    "\n",
    "In this case, we used `init='nndsvd'`, which stands for **Nonnegative Double Singular Value Decomposition (NNDSVD)**. This method provides a smart, data-driven initialization of the matrices `W` and `H`.\n",
    "\n",
    "- **Better convergence**: Good initialization helps the algorithm converge faster and more reliably.\n",
    "- **Improved results**: It avoids poor local minima that can occur with random starting values.\n",
    "- **More stability**: Especially useful on sparse datasets like user-item rating matrices.\n",
    "\n",
    "Using `nndsvd` is generally a better choice than the default `random`, especially in recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df41dd",
   "metadata": {},
   "source": [
    "Now let's calculate the by product of `W` and `H`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4de08fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "713dbb35",
   "metadata": {},
   "source": [
    "Now, as you can see, this matrix is dense and contains non-zero values that represent the predicted ratings.showing us `How a user would rate a book if they had read it.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4eeb9",
   "metadata": {},
   "source": [
    "Now let's define a method to recommend books to a user\n",
    "\n",
    "This method should generate top-N book recommendations for a given user.  \n",
    "So it needs to:\n",
    "- Identify books the user has already rated\n",
    "- Use the predicted ratings matrix to find unrated books with the highest predicted scores\n",
    "- Return the top-N recommended book IDs for that user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43967ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e929841",
   "metadata": {},
   "source": [
    "Now lets try it out with a random User ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d88863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b19a82c",
   "metadata": {},
   "source": [
    "We can see that if this user reads the book with ID `0312980140`, they would likely rate it above 6.3. This makes it a strong recommendation for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18b95c",
   "metadata": {},
   "source": [
    "Now imagine your boss has asked you:  \n",
    "**\"How would you evaluate the performance of this recommendation system?\"** ðŸ˜’\n",
    "\n",
    "A simple and effective way is to check how close the modelâ€™s predicted ratings are to the actual ratings that users have given â€” but only for the ratings we already know (ignoring the missing ones).\n",
    "\n",
    "This is where **Root Mean Squared Error (RMSE)** comes in. It measures the average difference between the predicted and actual ratings.\n",
    "\n",
    "A lower RMSE means the predictions are more accurate.\n",
    "\n",
    "Letâ€™s take a look at the code to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "720cfaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fd27f49",
   "metadata": {},
   "source": [
    "The average difference between the predicted and actual ratings is around `4.5`. ðŸ«¢  \n",
    "This suggests that our recommendation system isn't performing very well at the moment.\n",
    "\n",
    "Now how can we make it better?\n",
    "\n",
    "This is where **parameter optimization** comes in.\n",
    "\n",
    "In real-world scenarios, your first model is rarely the best one. But by tuning the modelâ€™s parameters, you can often make significant improvements.\n",
    "\n",
    "One of the key parameters we can adjust in **NMF** is the number of **latent features**, also known as `n_components`. This determines how many hidden patterns (e.g. user preferences or book characteristics) the model tries to learn.\n",
    "\n",
    "Letâ€™s see how different values of `n_components` affect the modelâ€™s accuracy. If you remember the current model has 2 `latent features`. Let's use a for loop to try different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d6aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb776190",
   "metadata": {},
   "source": [
    "Now let visualize `n_components vs error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0286a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8272b566",
   "metadata": {},
   "source": [
    "We can see that as we increase `n_components`, the error (RMSE) tends to decrease.  \n",
    "This means the model is capturing more subtle patterns in the data.\n",
    "\n",
    "But we should be careful â€” if `n_components` is too high:\n",
    "\n",
    "- The model may **overfit**: it memorizes the training data instead of learning general patterns.\n",
    "- **Sparse data** can't support too many latent features â€” leading to noisy, unstable `W` and `H` matrices.\n",
    "- **Training time increases** significantly with more components.\n",
    "- You might see **lower RMSE** on training data, but worse performance on unseen users/items.\n",
    "\n",
    "so by looking at this chart a good trade-off between performance and efficiency seems to be around **`n_components = 55`**.  \n",
    "It gives you a significantly lower RMSE compared to smaller values, without the added complexity of going all the way to 110."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4643a4",
   "metadata": {},
   "source": [
    "Amazing! \n",
    "\n",
    "You now have a working NMF-based recommender system and a basic understanding of model tuning and evaluation.  \n",
    "In future tutorials, weâ€™ll explore other use cases of NMF â€” starting with **topic modeling for text data**.\n",
    "\n",
    "Stay Tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
